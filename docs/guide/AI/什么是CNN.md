# 什么是CNN

CNN，Convolutional Neural Networks，卷积神经网络，是一种常用于图像处理和分析的深度学习模型，类似于一般的神经网络。对于HSIC，CNN作为一项已证明的对图像分析工作的有效的工具，无疑是一个值得尝试的方向。接下来的数个部分将从背景到应用的简略的介绍这项工具。



#### 1. 从Neural Network谈起 ——CNN的产生背景

卷积神经网络与传统人工神经网络的架构十分接近，可以视为传统神经网络的一种优化，所以在了解CNN之前要对其有一定了解。

##### 神经元模型

人工神经网络产生于对生物神经的模拟过程中，其单元是对生物神经元的模拟，具有运算和存储作用。这个人工神经元模型是一个包含输入，输出与计算功能的模型。神经网络便由这些单元构成。连接（connection）是相当于神经元中突触，负责数据的传输，每个连接上有一个权值（weight）每个经过这个连接的数据都会被加以这个权值（训练中改变的也主要是权值，目的是使权值调整到最佳，达到最佳的预测效果）。

节点接受数个加权后的输入，会产生输出，输出为
$$
f(\sum_iw_ix_i+b)
$$
其中$w_ix_i$是加权后的输入，$b$为偏置参数，是一个常量，$f(x)$是一个非线性函数，我们称之为激活函数（activation function）这个函数的作用是使结果可以为非线性的，从而使我们的神经网络可以学习更复杂的东西。常用的激活函数有relu和tanh等。

- **为什么要有激活函数？**

  神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系，如果不包括激活函数的话，那么它就单纯是一个矩阵的连乘，只是输入的线性变换，因此我们需要引入非线性的部分来使之可以拟合任意函数，这个非线性部分就是激活函数。

<img src="C:\Users\nitrophorus\Desktop\typora\image\v2-2e8d1a68d575f89c2fd471a25e69668d_r.png" alt="v2-2e8d1a68d575f89c2fd471a25e69668d_r" style="zoom:150%;" />

##### 单层神经网络

我们可以使用以上的神经元模型来构建神经网络。神经网络具有一个输入层和若干个计算层，单层神经网络，即仅具有一个计算层的神经网络，结构较为简单。

<img src="C:\Users\nitrophorus\Desktop\typora\image\aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvNjczNzkzLzIwMTUxMi82NzM3OTMtMjAxNTEyMzAyMDU0Mzc5OTUtNjczODU2NjQ0LmpwZw.png" alt="aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvNjczNzkzLzIwMTUxMi82NzM3OTMtMjAxNTEyMzAyMDU0Mzc5OTUtNjczODU2NjQ0LmpwZw" style="zoom: 50%;" />

如图，通过两个节点的计算，我们将输入进行加权得到了输出，上图中给出了计算公式，可以发现其能用矩阵乘法表示：
$$
\bold{a} = \begin{bmatrix} a_1 \\ a_2 \\ a_3\end{bmatrix} , \bold{W} = \begin{bmatrix} W_{1,1} & W_{1,2} & W_{1,3} \\ W_{2,1} & W_{2,2} & W_{2,3} \end{bmatrix} ,\bold{z} = \begin{bmatrix} z_1 \\ z_2 \end{bmatrix}
$$
$\bold{a}$表示输入向量，$\bold{z}$表示输出向量，$\bold{W}$表示权值矩阵，可以得到如下等式
$$
g(\bold{W} \times \bold{a}) = \bold{z}
$$
这个单层神经网络（或者说单层感知器）可以进行训练，有简单的线性分类功能，但是能力过弱，暂时按下不表，在多层神经网络中再做介绍。

##### 两层神经网络

含有两个计算层，在输出层外还含有一个隐含层，两层神经网络的拟合能力大大提高，理论上可以无限逼近任意连续函数。

<img src="C:\Users\nitrophorus\Desktop\typora\image\20200309102932420-1701219490305-2.png" alt="20200309102932420" style="zoom: 33%;" />

- **关于偏置**

  相当于给函数附加一个常量，使函数中心可以离开原点，拟合能力得到提高

- **关于训练**

  模型中的参数可以被训练以达到较好的效果

  需要一个指标来定义什么是好的预测，于是有了损失：
  $$
  loss = (yp - y)^2
  $$
  其中$yp$为预测目标，这个值可以表征预测的好坏，训练的目的可以转换为使对所有训练数据的损失和尽可能小。将矩阵公式代入后，损失可以写为关于参数的函数，可以将问题转化为一个优化问题：如何优化参数能让损失函数的值最小。

#### 2. 从神经网络到卷积神经网络

![f539b30a49e44cbf9a98041ff87f7fe5](C:\Users\nitrophorus\Desktop\typora\image\f539b30a49e44cbf9a98041ff87f7fe5.png)

卷积神经网络可以看作传统神经网络的一种演进，主要的区别在于全连接和卷积。

##### 目的：减少参数

传统神经网络的每一个层次都是全连接层，每一层的网络都和前一层每一个系数都有关系，所以会有极大量的参数。而卷积神经网络的卷积层使用区域相关的思路，每个卷积层单元仅连接输入的一部分，参数数量较少。

![v2-cacf107a0398c3699b7a0e3bee27663c_1440w](C:\Users\nitrophorus\Desktop\typora\image\v2-cacf107a0398c3699b7a0e3bee27663c_1440w.jpg)

##### 卷积神经网络的结构

- 卷积层

  作为CNN中最重要的层次，在卷积层中，输入图像与一组可学习的卷积核（也称为滤波器，实质上与传统神经网络的节点相似）进行卷积操作。

  <img src="C:\Users\nitrophorus\Desktop\typora\image\d336edeee33e4cb59d9aadad0283463a.png" alt="d336edeee33e4cb59d9aadad0283463a" style="zoom: 67%;" />

  按原理说，对于每个区域都应有一个对应的卷积核，但实际情况中，通常一个卷积核会对整张图产生作用，出现这种情况的原因是**参数共享**，这是CNN的一个基本特征。参数共享的实质是：所有的filter共用一组权值，这大大减少了参数的数量，从而卷积层的操作可以看成一个filter在输入上的滑动。可以进行这种操作的原因是：**图片的底层特征是与特征在图片中的位置无关的**。卷积层是用于提取纹理，颜色等底层信息的，它们的特征与位置无关，所以对于不同的位置使用不同的权值是没有必要的。这样做以后，对于任意大小的图片都只需要固定数量的参数，这个数量要远小于全连接网络的参数数量。

  卷积运算的目的是提取输入的不同特征，某些卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多卷积层能够从低级特征中迭代提取更复杂的特征。

  卷积过程实质上就是两个矩阵做乘法。在卷积过程后，原始输入矩阵会有一定程度的缩小，比如定义卷积核大小为3*3，步长为1时，矩阵长宽会缩小2，所以在一些应用场合下，为了保持输入矩阵的大小，我们在卷积操作前需要对数据进行扩充，常见的扩充方法为0填充。

  <img src="C:\Users\nitrophorus\Desktop\typora\image\7d8ea6cc18f447b88bf05b07eaf04957.png" alt="7d8ea6cc18f447b88bf05b07eaf04957" style="zoom: 33%;" />

  填充也用于保护边缘内容。如果不进行填充，一些边缘的内容可能被忽略。

  <img src="C:\Users\nitrophorus\Desktop\typora\image\9789b3741ec54bc5ad4f7a3e006065a0.png" alt="9789b3741ec54bc5ad4f7a3e006065a0" style="zoom:33%;" />

  卷积神经网络中也存在偏置向量，作用与传统神经网络中的相同。

- 激活函数

  与传统神经网络中的类似，激活函数可以增加网络的非线性能力

  ![OIP-C (1)](C:\Users\nitrophorus\Desktop\typora\image\OIP-C (1).jpg)

  最常用的为ReLU（Rectified Linear Units layer）

- 池化层

  池化层用于减小特征图的空间维度，减少参数数量，它的本质实际上就是采样。对于输入的 Feature Map（由Convolutional Layers产生），选择某种方式对其进行降维压缩，以加快运算速度。

  池化层有如下作用：

  - 特征降维

    图像含有的信息量很大，特征也很多，但是有些信息对于我们的任务没有太多用途或者有重复，可以把这类冗余信息去除，把最重要的特征抽取出来。

  - 在一定程度上防止过拟合。

    由于部分参数被舍弃了，一部分特征被因此也被舍弃，这样在一定程度上防止了过拟合。

  因为池化综合了（filter范围内的）全部邻居的反馈，即通过k个像素的统计特性而不是单个像素来提取特征，自然这种方法能够大大提高神经网络的性能。

  **为什么可以这样操作？**

  局部线性变换具有不变性，如果输入数据的局部进行了线性变换操作（如平移或旋转等），那么经过池化操作后，输出的结果并不会发生变化。

  最常用的池化操作是最大池化（Max Pooling），平均池化（Average Pooling）也经常被使用。

  ![8e57fcc0c57a4dada9b58fc550bfef37](C:\Users\nitrophorus\Desktop\typora\image\8e57fcc0c57a4dada9b58fc550bfef37.png)

- 全连接层

  ![下载 (2)](C:\Users\nitrophorus\Desktop\typora\image\下载 (2).jpg)

  以上，已经通过数个卷积层和池化层完成了特征提取的工作，只需要进行分类了。全连接层的作用就是完成分类工作，分类工作的第一步是把我们获得的分布式特征表示（体现在卷积的最终结果中）的信息在节点中共享。

  从图示中可以注意到，图中的全连接层由两层平铺的4096个节点构成，那么就出现一个问题：如何从最终卷积结果变到1\*1\*4096的向量表示，并共享它们的信息。如果输入为7\*7\*512，按CNN的方法，我们可以将它们用卷积核为7\*7\*512\*4096的全局卷积来实现这个全连接运算。
  
  这一步完成了将特征整合并映射到样本空间。
  
  可以有多层全连接层，会使拟合效果更好，但是会增加过拟合风险和运算负担。
  
  **近期的成果：**
  
  FC可以用GAP的方法代替
  
  <img src="C:\Users\nitrophorus\Desktop\typora\image\971a2e67797e44c1a0c1930c00e08cad.png" alt="971a2e67797e44c1a0c1930c00e08cad" style="zoom: 67%;" />
  
  如果要预测K个类别，在卷积特征抽取部分的最后一层卷积层，就会生成K个特征图，然后通过全局平均池化就可以得到 K个1×1的特征图，将这些1×1的特征图输入到softmax layer之后，每一个输出结果代表着这K个类别的概率（或置信度 confidence），起到取代全连接层的效果。
  
  - 和全连接层相比，使用全局平均池化技术，对于建立特征图和类别之间的关系，是一种更朴素的卷积结构选择。
  - 全局平均池化层不需要参数，避免在该层产生过拟合。
  - 全局平均池化对空间信息进行求和，对输入的空间变化的鲁棒性更强。

##### 如何训练CNN

CNN的训练过程和传统的全连接神经网络类似，损失函数的定义与其不同，一般使用交叉熵

<img src="C:\Users\nitrophorus\Desktop\typora\image\v2-fb00b046eb39163b75ca3c00eabcd626_r.png" alt="v2-fb00b046eb39163b75ca3c00eabcd626_r" style="zoom:50%;" />

全连接层之后实际上有一层softmax层，有激励函数如下：
$$
softmax(z_i)=\frac{e^{z_i}}{\sum^C_{c=1}e^{z_c}}
$$
其中 $z_i$ 为第i个节点的输出值，$C$为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在[0, 1]和为1的概率分布。

<img src="C:\Users\nitrophorus\Desktop\typora\image\20190206210834942.png" alt="20190206210834942" style="zoom: 33%;" />

于是有损失函数如下：
$$
Loss = \frac1N\sum_iLoss_i = -\frac1N\sum_i\sum_{c=1}^My_{ic}\log{(p_{ic})}
$$
其中$M$为类别的数量，$y_{ic}$为符号函数，值为$0$或$1$，如果样本$i$的真实类别等于$c$取$1$ ，否则取$0$，$p_{ic}$为观测样本$i$属于类别 ![c](https://www.zhihu.com/equation?tex=c&consumer=ZHI_MENG) 的预测概率。

训练过程和传统神经网络基本一致，都是通过梯度下降法找出最优的$(\bold{W},\bold{b})$，使loss最小。
